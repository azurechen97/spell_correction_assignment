{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation\n",
    "\n",
    "As the text is in category of news, we can use the Reuters corpus in **ntlk**.\n",
    "\n",
    "First, we extract the word frequency, bigram and other informations from the corpus. To save time, we can use **pickle** library to save the data into an external file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import math\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import reuters\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters_list = reuters.words()\n",
    "un_freq = nltk.FreqDist(reuters_list) # word frequency\n",
    "bigrams = nltk.bigrams(reuters_list)\n",
    "bi_freq = nltk.FreqDist(bigrams) # Bigram frequency\n",
    "V = len(un_freq) # vocabulary size\n",
    "N = len(reuters_list) # tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data into dataFile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataFile','wb') as fw:\n",
    "    pickle.dump(un_freq,fw)\n",
    "    pickle.dump(bi_freq,fw)\n",
    "    pickle.dump(V,fw)\n",
    "    pickle.dump(N,fw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open a new python file for main program. Firstly open the vocabulary file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.txt') as vocab_file:\n",
    "    lines = vocab_file.readlines()\n",
    "vocab = [line.strip() for line in lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the saved data from dataFile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataFile','rb') as fr:\n",
    "    un_freq = pickle.load(fr)\n",
    "    bi_freq = pickle.load(fr)\n",
    "    V = pickle.load(fr)\n",
    "    N = pickle.load(fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = pd.read_table('testdata.txt', header=None)\n",
    "n = testdata.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the candidate words\n",
    "\n",
    "If we calculate each word's edit distance to every word in the vocaburary, it will take a huge amount of time. Thus we can first generate a set of possible words with edit distance of 1 to each misspelled word, then check if they are legal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "letters_upper = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "other = \" -'\"\n",
    "\n",
    "def distance_1(word):\n",
    "    total = {word} #Including itself\n",
    "    for i in range(0,len(word)):\n",
    "        l = word[:i]\n",
    "        r = word[i:]\n",
    "        alphabet = letters\n",
    "        if len(word) > 1 and i > 0:\n",
    "            total.update(set([l[:i-1]+r[0]+l[i-1]+r[1:]])) #Transposition\n",
    "            if i < len(word):\n",
    "                alphabet = letters+other # Punctuation cases\n",
    "        total.update(set([l+c+r for c in alphabet]+[l+r+c for c in alphabet]))\n",
    "        #Insertion\n",
    "        total.update(set([l+r[1:]])) #Deletion\n",
    "        total.update(set([l+c+r[1:] for c in alphabet])) #Substitution\n",
    "    total = total - {\"\"} #Remove empty string\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For words with uppercase letter, we can take the short cuts:\n",
    "* If the word contains one uppercase letter that is not in the initial position, we can try to swap the letter with the current initial letter;\n",
    "* If the word only have one lowercase letter, we can consider deleting the letter or substitude the letter with an uppercase one;\n",
    "* In other cases, if the word have more than one uppercase letter, it's reasonable to insert or substitute an uppercase letter into the misspelled word;\n",
    "* If none of the mentioned operation can generate a legal word, then take the word as in the normal situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_upper(word): #Output the number and position of uppercase letters\n",
    "    count = 0\n",
    "    pos = []\n",
    "    for i in range(0,len(word)):\n",
    "        if word[i].isupper():\n",
    "            count += 1\n",
    "            pos.append(i)\n",
    "    return [count,pos]\n",
    "\n",
    "def uppercase_corr(word):\n",
    "    total = {word}\n",
    "     #UPPERCASES\n",
    "    find = find_upper(word)\n",
    "    num = find[0]\n",
    "    ind = find[1]\n",
    "    if num == 1 and ind[0] == 0:\n",
    "        return {word}\n",
    "    elif num == 1 and ind[0] > 0: #One uppercase\n",
    "        new_word = word[ind[0]]+word[1:ind[0]]+word[0]+word[ind[0]+1:]\n",
    "        return {new_word,word}\n",
    "    elif num == len(word)-1: #e.g. INTERNAsIONAL\n",
    "        for i in range(0,len(word)):\n",
    "            if i not in ind:\n",
    "                l = word[:i]\n",
    "                r = word[i:]\n",
    "                alphabet = letters_upper\n",
    "                if len(word) > 1 and i > 0:\n",
    "                    if i < len(word):\n",
    "                        alphabet = letters_upper+other # Punctuation cases\n",
    "                total.update(set([l+r[1:]])) #Deletion\n",
    "                total.update(set([l+c+r[1:] for c in alphabet])) #Substitution\n",
    "                break\n",
    "        return total\n",
    "    else: #e.g. INTERVNTION ltGR\n",
    "        for i in range(0,len(word)):\n",
    "            l = word[:i]\n",
    "            r = word[i:]\n",
    "            alphabet = letters_upper\n",
    "            if len(word) > 1 and i > 0:\n",
    "                #total.update(set([l[:i-1]+r[0]+l[i-1]+r[1:]])) #Transposition\n",
    "                if i < len(word):\n",
    "                    alphabet = letters_upper+other # Punctuation cases\n",
    "            total.update(set([l+c+r for c in alphabet]+[l+r+c for c in alphabet]))\n",
    "            #Insertion\n",
    "            #total.update(set([l+r[1:]])) #Deletion\n",
    "            total.update(set([l+c+r[1:] for c in alphabet])) #Substitution\n",
    "        return total\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If still no legal word is generated, we must consider the situation that the edit distance is more than 1.\n",
    "\n",
    "When the edit distance is 2, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_2(word):\n",
    "    return set(d2 for d1 in distance_1(word) for d2 in distance_1(d1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it's easy to find the lack of efficiency in this method. Observing the testing text, we find that the situation where edit distance is more than 1 are mostly just swapped two of the letters in the word. So we can use the function below instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_distance_1(word): #Swap\n",
    "    total = set()\n",
    "    for i in range(0,len(word)-1):\n",
    "        for j in range(i+1,len(word)):\n",
    "            if word[i] != word[j]:\n",
    "                new_word = word[:i]+word[j]+word[i+1:j]+word[i]+word[j+1:]\n",
    "                total.add(new_word)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the probability\n",
    "\n",
    "We use the frequency of unigrams and bigrams to approximately calculate the emerging probability of the next word, to choose a word most likely to appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_smoothed_prob(pre,next):\n",
    "    prob = math.log10((bi_freq[(pre,next)]+1))-math.log10((un_freq[pre]+V))\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prevent underflow, we take the logarithm of the probabilities. In addition, use Laplace Smothing to roughly handle the zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correction: non-word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction(pre,word,vocab): #take the previous word as parameter as well\n",
    "    prob_dict = dict()\n",
    "    find = find_upper(word) # find uppercase\n",
    "    num = find[0]\n",
    "    if num > 0: #with uppercase\n",
    "        for y in uppercase_corr(word):\n",
    "            if y in vocab:\n",
    "                prob_dict[y]=log_smoothed_prob(pre,y)\n",
    "    for y in distance_1(word): #normal situation\n",
    "        if y in vocab:\n",
    "            prob_dict[y]=log_smoothed_prob(pre,y)\n",
    "    for y in non_distance_1(word): #swapping situation\n",
    "            if y in vocab:\n",
    "                prob_dict[y]=log_smoothed_prob(pre,y)\n",
    "    if len(prob_dict) == 0:\n",
    "        return word #if still no candidates generated, give up\n",
    "    else:\n",
    "        return max(prob_dict,key=prob_dict.get)\n",
    "        #output the one with greatest probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the functions to correct the words!\n",
    "\n",
    "Meanwhile, record the wrong words we can't detect at the moment for further investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 protectionst protectionist\n",
      "2 Tkyo Tokyo\n",
      "3 retaiation retaliation\n",
      "4 tases taxes\n",
      "5 busines business\n",
      "7 Taawin Taiwan\n",
      "8 seriousnyss seriousness\n",
      "9 aganst against\n",
      "10 bililon billion\n",
      "11 sewll swell\n",
      "12 importsi imports\n",
      "13 Sheem Sheen\n",
      "14 wsohe whose\n",
      "15 Koreva Korea\n",
      "16 Japn Japan\n",
      "17 semicondctors semiconductors\n",
      "18 advantagne advantage\n",
      "19 Lawrenc Lawrence\n"
     ]
    }
   ],
   "source": [
    "exist_real_word_errors = list()\n",
    "result = testdata.drop(columns=1)\n",
    "for i in range(0,n):\n",
    "    non_word_count = 0\n",
    "    sentence = word_tokenize(testdata[2][i])\n",
    "    for p, word in enumerate(sentence):\n",
    "        if non_word_count == testdata[1][i]:\n",
    "            #no need to loop when the number is enough\n",
    "           break\n",
    "        if word not in vocab:\n",
    "            non_word_count += 1\n",
    "            correct_word = correction(sentence[p-1],word,vocab)\n",
    "            if i<20: #only print the head\n",
    "                print(str(i+1)+\" \"+word+\" \"+correct_word)\n",
    "            result.iat[i,1] = result.iat[i,1].replace(word,correct_word)\n",
    "    if non_word_count != testdata[1][i]:\n",
    "        exist_real_word_errors.append(i) #real word error positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the position with read word errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 19, 47, 54, 64, 118, 123, 138, 153, 157, 170, 177, 265, 267, 294, 356, 391, 410, 435, 471, 493, 497, 534, 538, 553, 619, 620, 635, 681, 684, 768, 783, 801, 813, 819, 843, 860, 869, 875, 901, 911, 931, 932, 945, 954, 955, 968, 979, 987]\n"
     ]
    }
   ],
   "source": [
    "print(exist_real_word_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correction: real-word\n",
    "\n",
    "We first instruct a function to generate candidate sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates(sentence):\n",
    "    candidates = list([sentence]) #contains itself\n",
    "    for p,word in enumerate(sentence):\n",
    "        if word[0] not in letters+letters_upper:\n",
    "            #let go of the one with punctuations at initial\n",
    "            continue\n",
    "        elif word[0] in letters_upper: # uppercase initial\n",
    "            for replace in distance_1(word.lower()):\n",
    "                if len(replace) == 1: # one letter word\n",
    "                    replace = replace[0].upper()\n",
    "                else:\n",
    "                    replace = replace[0].upper()+replace[1:]\n",
    "                if replace in vocab:\n",
    "                    candidate = sentence[:p]+[replace]+sentence[p+1:]\n",
    "                    if candidate not in candidates: # avoid repitition\n",
    "                        candidates.append(candidate)\n",
    "        else: #normal\n",
    "            for replace in distance_1(word):\n",
    "                if replace in vocab:\n",
    "                    candidate = sentence[:p]+[replace]+sentence[p+1:]\n",
    "                    if candidate not in candidates:\n",
    "                        candidates.append(candidate)\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the approximate probability of each possible sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_prob(sentence):\n",
    "    log_prob = 0\n",
    "    for p,word in enumerate(sentence):\n",
    "        if p == 0:\n",
    "            log_prob = math.log10(un_freq[word]+1) - math.log10(N+V)\n",
    "        else:\n",
    "            pre = sentence[p-1]\n",
    "            new_prob = log_smoothed_prob(pre,word)\n",
    "            log_prob = log_prob + new_prob\n",
    "    return log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting the most proper sentence in the candidates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_word_correction(sentence):\n",
    "    max_prob = -10000\n",
    "    best_candidate = sentence\n",
    "    for candidate in generate_candidates(sentence):\n",
    "        prob = sentence_prob(candidate)\n",
    "        if prob > max_prob:\n",
    "            max_prob = prob\n",
    "            best_candidate = candidate\n",
    "    return best_candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 pace place\n",
      "20 whoe whole\n",
      "48 alter after\n",
      "55 taking making\n",
      "65 trade traded\n",
      "119 so to\n",
      "124 mouth month\n",
      "139 latter later\n",
      "154 boots boost\n",
      "158 rule ruled\n",
      "171 trades trade\n",
      "178 sill still\n",
      "266 consume consumer\n",
      "268 markets market\n",
      "295 stacks stocks\n"
     ]
    }
   ],
   "source": [
    "for i in exist_real_word_errors:\n",
    "    sentence = word_tokenize(result.iat[i,1])\n",
    "    correct_sentence = real_word_correction(sentence)\n",
    "    for j in range(0,len(sentence)):\n",
    "        if sentence[j] != correct_sentence[j]:\n",
    "            word = sentence[j]\n",
    "            correct_word = correct_sentence[j]\n",
    "            result.iat[i,1] = result.iat[i,1].replace(word,correct_word)\n",
    "            break\n",
    "    if i<300:\n",
    "        print(str(i+1)+\" \"+word+\" \"+correct_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the corrected sentences into result.txt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('result.txt',result.values,fmt='%s',delimiter='\\t',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Use **eval.py** to calculate the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is : 96.80%\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "anspath='./ans.txt'\n",
    "resultpath='./result.txt'\n",
    "ansfile=open(anspath,'r')\n",
    "resultfile=open(resultpath,'r')\n",
    "count=0\n",
    "for i in range(1000):\n",
    "    ansline=ansfile.readline().split('\\t')[1]\n",
    "    ansset=set(nltk.word_tokenize(ansline))\n",
    "    resultline=resultfile.readline().split('\\t')[1]\n",
    "    resultset=set(nltk.word_tokenize(resultline))\n",
    "    if ansset==resultset:\n",
    "        count+=1\n",
    "print(\"Accuracy is : %.2f%%\" % (count*1.00/10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
