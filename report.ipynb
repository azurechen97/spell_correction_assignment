{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import math\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.txt') as vocab_file:\n",
    "    lines = vocab_file.readlines()\n",
    "vocab = [line.strip() for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-915af4dbaf57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "vocab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = pd.read_table('testdata.txt', header=None)\n",
    "n = testdata.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "un_freq = nltk.FreqDist(reuters.words())\n",
    "V = len(un_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = nltk.bigrams(reuters.words())\n",
    "bi_freq = nltk.FreqDist(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "other = \" -'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_1(word):\n",
    "    total = {word}\n",
    "    for i in range(0,len(word)):\n",
    "        l = word[:i]\n",
    "        r = word[i:]\n",
    "        alphabet = letters\n",
    "        if len(word) > 1 and i > 0:\n",
    "            total.update(set([l[:i-1]+r[0]+l[i-1]+r[1:]])) #Transposition\n",
    "            if i < len(word):\n",
    "                alphabet = letters+other\n",
    "        total.update(set([l+c+r for c in alphabet]+[l+r+c for c in alphabet])) #Insertion\n",
    "        total.update(set([l+r[1:]])) #Deletion\n",
    "        total.update(set([l+c+r[1:] for c in alphabet])) #Substitution\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_2(word):\n",
    "    return set(d2 for d1 in distance_1(word) for d2 in distance_1(d1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_smoothed_prob(pre,next):\n",
    "    prob = math.log10((bi_freq[(pre,next)]+1))-math.log10((un_freq[pre]+V))\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction(pre,word,vocab):\n",
    "    prob_dict = dict()\n",
    "    for y in distance_1(word):\n",
    "        if y in vocab:\n",
    "            prob_dict[y]=log_smoothed_prob(pre,y)\n",
    "    if len(prob_dict) == 0:\n",
    "        return \"Not Found\"\n",
    "    else:\n",
    "        return max(prob_dict,key=prob_dict.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protectionst protectionist\n",
      "Tkyo Tokyo\n",
      "retaiation retaliation\n",
      "tases taxes\n",
      "busines business\n",
      "Taawin Not Found\n",
      "seriousnyss seriousness\n",
      "aganst against\n",
      "bililon billion\n",
      "sewll swell\n",
      "importsi imports\n",
      "Sheem Sheen\n",
      "wsohe Not Found\n",
      "Koreva Korea\n",
      "Japn Japan\n",
      "semicondctors semiconductors\n",
      "advantagne advantage\n",
      "Lawrenc Lawrence\n",
      "disadxantage disadvantage\n",
      "rxpoet Not Found\n",
      "conceern concern\n",
      "amtter matter\n",
      "cenntred centred\n",
      "trad trade\n",
      "Liberala Liberal\n",
      "inoclude include\n",
      "fcsial Not Found\n",
      "Representetive Representative\n",
      "ootput output\n",
      "methids methods\n",
      "producton production\n",
      "parep prep\n",
      "endergy energy\n",
      "Japnese Japanese\n",
      "Agencay Agency\n",
      "energyr energy\n",
      "pfovided provided\n",
      "imhorts imports\n",
      "incease increase\n",
      "quartee quarter\n",
      "expanted expanded\n",
      "Exeport Export\n",
      "clotcing clothing\n",
      "arnuod Not Found\n",
      "intehnational international\n",
      "markats markets\n",
      "aiagnst Not Found\n",
      "movament movement\n",
      "dotay Not Found\n",
      "racgo Not Found\n",
      "Coincul Not Found\n",
      "affecced affected\n",
      "evfect effect\n",
      "tobacoc tobacco\n",
      "rithee Not Found\n",
      "beng being\n",
      "rubbes rubber\n",
      "cautiousy cautiously\n",
      "safly safely\n",
      "Physicay Physical\n",
      "Rtbusoa Not Found\n",
      "rubebr rubber\n",
      "afte after\n",
      "Salh Saleh\n",
      "Nannggolai Not Found\n",
      "Amerrican American\n",
      "althlugh although\n",
      "locayll Not Found\n",
      "bilion billion\n",
      "broers brokers\n",
      "shepment shipment\n",
      "ltuRA Not Found\n",
      "Rivir River\n",
      "produc produce\n",
      "abouct about\n",
      "lovans loans\n",
      "Reutres Reuters\n",
      "inserview interview\n",
      "wihle while\n",
      "cuiqkly Not Found\n",
      "Somitumo Not Found\n",
      "conparable comparable\n",
      "agdeer Not Found\n",
      "ovesreas overseas\n",
      "plae place\n",
      "Klieinwort Kleinwort\n",
      "icrease increase\n",
      "Suoitomm Not Found\n",
      "Kmatsu Komatsu\n",
      "viraous Not Found\n",
      "thtree three\n",
      "recusities Not Found\n",
      "aapJn Not Found\n",
      "Kmatsu Komatsu\n",
      "businiss business\n"
     ]
    }
   ],
   "source": [
    "exist_real_word_errors = list()\n",
    "for i in range(0,100):\n",
    "    misspell_count = 0\n",
    "    sentence = word_tokenize(testdata[2][i])\n",
    "    for p, word in enumerate(sentence):\n",
    "        if misspell_count == testdata[1][i]:\n",
    "           break\n",
    "        if word not in vocab:\n",
    "            misspell_count += 1\n",
    "            print(word+\" \"+correction(sentence[p-1],word,vocab))\n",
    "    if misspell_count != testdata[1][i]:\n",
    "        exist_real_word_errors.append([i, testdata[1][i] - misspell_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction(pre,word,vocab):\n",
    "    prob_dict = dict()\n",
    "    for y in distance_1(word):\n",
    "        if y in vocab:\n",
    "            prob_dict[y]=log_smoothed_prob(pre,y)\n",
    "    if len(prob_dict) == 0:\n",
    "        for y in distance_2(word):\n",
    "            if y in vocab:\n",
    "                prob_dict[y]=log_smoothed_prob(pre,y)\n",
    "    if len(prob_dict) == 0:\n",
    "        return \"Not Found\"\n",
    "    else:\n",
    "        return max(prob_dict,key=prob_dict.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protectionst protectionist\n",
      "Tkyo Tokyo\n",
      "retaiation retaliation\n",
      "tases taxes\n",
      "busines business\n",
      "Taawin Taiwan\n",
      "seriousnyss seriousness\n",
      "aganst against\n",
      "bililon billion\n",
      "sewll swell\n",
      "importsi imports\n",
      "Sheem Sheen\n",
      "wsohe some\n",
      "Koreva Korea\n",
      "Japn Japan\n",
      "semicondctors semiconductors\n",
      "advantagne advantage\n",
      "Lawrenc Lawrence\n",
      "disadxantage disadvantage\n",
      "rxpoet export\n",
      "conceern concern\n",
      "amtter matter\n",
      "cenntred centred\n",
      "trad trade\n",
      "Liberala Liberal\n",
      "inoclude include\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-3af20253642e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mmisspell_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcorrection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmisspell_count\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtestdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mexist_real_word_errors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmisspell_count\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-d876a0a735f5>\u001b[0m in \u001b[0;36mcorrection\u001b[0;34m(pre, word, vocab)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_dict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistance_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m                 \u001b[0mprob_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_smoothed_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_dict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "exist_real_word_errors = list()\n",
    "for i in range(0,100):\n",
    "    misspell_count = 0\n",
    "    sentence = word_tokenize(testdata[2][i])\n",
    "    for p, word in enumerate(sentence):\n",
    "        if misspell_count == testdata[1][i]:\n",
    "           break\n",
    "        if word not in vocab:\n",
    "            misspell_count += 1\n",
    "            print(word+\" \"+correction(sentence[p-1],word,vocab))\n",
    "    if misspell_count != testdata[1][i]:\n",
    "        exist_real_word_errors.append([i, testdata[1][i] - misspell_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def correction(pre,word,vocab):\n",
    "    prob_dict = dict()\n",
    "    for y in distance_1(word):\n",
    "        if y in vocab:\n",
    "            prob_dict[y]=log_smoothed_prob(pre,y)\n",
    "    if len(prob_dict) == 0:\n",
    "        for y in distance_2(word):\n",
    "            if y in vocab and bi_freq[(pre,y)] > 0:\n",
    "                return y\n",
    "    if len(prob_dict) == 0:\n",
    "        return \"Not Found\"\n",
    "    else:\n",
    "        return max(prob_dict,key=prob_dict.get)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protectionst protectionist\n",
      "Tkyo Tokyo\n",
      "retaiation retaliation\n",
      "tases taxes\n",
      "busines business\n",
      "Taawin Taiwan\n",
      "seriousnyss seriousness\n",
      "aganst against\n",
      "bililon billion\n",
      "sewll swell\n",
      "importsi imports\n",
      "Sheem Sheen\n",
      "wsohe she\n",
      "Koreva Korea\n",
      "Japn Japan\n",
      "semicondctors semiconductors\n",
      "advantagne advantage\n",
      "Lawrenc Lawrence\n",
      "disadxantage disadvantage\n",
      "rxpoet export\n",
      "conceern concern\n",
      "amtter matter\n",
      "cenntred centred\n",
      "trad trade\n",
      "Liberala Liberal\n",
      "inoclude include\n",
      "fcsial fiscal\n",
      "Representetive Representative\n",
      "ootput output\n",
      "methids methods\n",
      "producton production\n",
      "parep prep\n",
      "endergy energy\n",
      "Japnese Japanese\n",
      "Agencay Agency\n",
      "energyr energy\n",
      "pfovided provided\n",
      "imhorts imports\n",
      "incease increase\n",
      "quartee quarter\n",
      "expanted expanded\n",
      "Exeport Export\n",
      "clotcing clothing\n",
      "arnuod around\n",
      "intehnational international\n",
      "markats markets\n",
      "aiagnst against\n",
      "movament movement\n",
      "dotay today\n",
      "racgo range\n",
      "Coincul Council\n",
      "affecced affected\n",
      "evfect effect\n",
      "tobacoc tobacco\n",
      "rithee either\n",
      "beng being\n",
      "rubbes rubber\n",
      "cautiousy cautiously\n",
      "safly safely\n",
      "Physicay Physical\n",
      "Rtbusoa Robusta\n",
      "rubebr rubber\n",
      "afte after\n",
      "Salh Saleh\n"
     ]
    }
   ],
   "source": [
    "exist_real_word_errors = list()\n",
    "for i in range(0,100):\n",
    "    misspell_count = 0\n",
    "    sentence = word_tokenize(testdata[2][i])\n",
    "    for p, word in enumerate(sentence):\n",
    "        if misspell_count == testdata[1][i]:\n",
    "           break\n",
    "        if word not in vocab:\n",
    "            misspell_count += 1\n",
    "            print(word+\" \"+correction(sentence[p-1],word,vocab))\n",
    "    if misspell_count != testdata[1][i]:\n",
    "        exist_real_word_errors.append([i, testdata[1][i] - misspell_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
